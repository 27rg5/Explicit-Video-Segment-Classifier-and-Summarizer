# -*- coding: utf-8 -*-
"""LDA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fM09p2DY-aDP1IYWcU5RwbR4jSKBkfkp

## Import packages
"""

import pandas as pd
from bertopic import BERTopic
from gensim.models import TfidfModel
import joblib
import shutil
from joblib import dump, load
from sklearn.model_selection import ParameterGrid
import json
import os
import numpy
import pyLDAvis
import pyLDAvis.gensim
import numpy as np
import pandas as pd
import re
import time
import logging
from multiprocessing import Pool
from joblib import Parallel, delayed
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split, GridSearchCV, PredefinedSplit,RandomizedSearchCV
from sklearn.metrics import accuracy_score, f1_score
from gensim.models.coherencemodel import CoherenceModel
import warnings

# Suppress all warnings
warnings.filterwarnings('ignore')

def setup_logger(logger_name, model_topic_param_dir,  level=logging.INFO):
    log_format = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s : %(message)s')

    log_file = os.path.join(model_topic_param_dir, f'gridsearch_logs.log')
    handler = logging.FileHandler(log_file)
    handler.setFormatter(log_format)

    logger = logging.getLogger(logger_name)
    logger.setLevel(level)
    logger.addHandler(handler)

    return logger

"""## Read file containing all videos captioned"""

df = pd.read_csv("/home/shaunaks/Explicit-Video-Segment-Classifier-and-Summarizer/runs/attention_fusion_default_networks_pairwise_21epochs/lda_preds.csv")

"""## Preprocess Data"""

df['Caption'] = \
df['Caption'].map(lambda x: re.sub('[,\.!?]', '', x))
df['Caption'] = \
df['Caption'].map(lambda x: x.lower())
df['Caption'].head()

import gensim
from gensim.utils import simple_preprocess
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
stop_words = stopwords.words('english')
#print(stop_words)

def sent_to_words(sentences):
    for sentence in sentences:
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))
def remove_stopwords(texts, keep_sentence=False):
    if not keep_sentence:
        return [[word for word in simple_preprocess(str(doc)) 
             if word not in stop_words] for doc in texts]
    else:
        return [' '.join([word for word in simple_preprocess(str(doc)) 
             if word not in stop_words]) for doc in texts]

data = df.Caption.values.tolist()
data_words = list(sent_to_words(data))
data_words = remove_stopwords(data_words)

import plotly.graph_objects as go
import gensim.corpora as corpora


id2word = corpora.Dictionary(data_words)
texts = data_words
import collections
from collections import defaultdict
word_to_count = collections.Counter()

for i, text in enumerate(texts):
    temp_dict = collections.Counter(text)
    word_to_count.update(temp_dict)
word_to_count = dict(sorted(word_to_count.items(), key=lambda item: item[1], reverse=True))
corpus = [id2word.doc2bow(text) for text in texts]

words = list(word_to_count.keys())
count_ = list(word_to_count.values())


"""## Visualize LDA on corpus using only 10 topics"""

num_topics = 10
lda_model_copy = gensim.models.LdaMulticore(corpus=corpus,
                                       id2word=id2word,
                                       num_topics=num_topics,
                                      random_state = np.random.RandomState(42))

lda_model_copy_asymmetric = gensim.models.LdaMulticore(corpus=corpus,
                                       id2word=id2word,
                                       num_topics=num_topics,
                                       alpha='asymmetric',
                                       eta='auto',
                                      random_state = np.random.RandomState(42))

data = df.Caption.values.tolist()
data_words = list(sent_to_words(data))
data_words = remove_stopwords(data_words, True)


#def find_best_topic(corpus, id2word, data_words):
max_topic = None
max_coherence = -1

#def get_coherence(topic):

# pool = Parallel(n_jobs=-1)
# topic_range = range(1, 150)
# all_results = pool(delayed(get_coherence)(topic) for topic in topic_range)

for topic in range(1, 150):
    import pdb;pdb.set_trace()
    lda_model = gensim.models.LdaMulticore(corpus=corpus,
                                        id2word=id2word,
                                        num_topics=topic,
                                            random_state = np.random.RandomState(42))#, workers=1)

    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_words, dictionary=id2word, coherence='c_v')#, processes=1)
    coherence_lda = coherence_model_lda.get_coherence()
    print(f'Coherence Score: {coherence_lda} and num_topics: {topic}')    
#    return topic, coherence_lda 

    if coherence_lda > max_coherence:
        max_coherence = coherence_score
        max_topic = topic
# for topic, coherence_score in all_results:
#     if coherence_score > max_coherence:
#         max_coherence = coherence_score
#         max_topic = topic
print(f'Max topic:{max_topic} with coherence score: {max_coherence}')
#    return max_topic, max_coherence

#best_num_topics,_ = find_best_topic(corpus, id2word, data_words)
lda_model = gensim.models.LdaMulticore(corpus=corpus,
                                       id2word=id2word,
                                       num_topics=best_num_topics,
                                      random_state = np.random.RandomState(42))

#Visualize using mmds as multidimentionality reduction technique



lda_model_asymmetric = gensim.models.LdaMulticore(corpus=corpus,
                                       id2word=id2word,
                                       num_topics=max_topic,
                                       alpha = 'asymmetric',
                                        eta='auto',
                                      random_state = np.random.RandomState(42))

lda_model_more_topics = gensim.models.LdaMulticore(corpus=corpus,
                                       id2word=id2word,
                                       num_topics=150,
                                      random_state = np.random.RandomState(42))


# tfidf = TfidfModel(corpus=corpus, id2word=id2word)  # fit model
# tf_idf_corpus = tfidf[corpus]

# best_num_topics, _ = find_best_topic(tf_idf_corpus, id2word, data_words)
# lda_model_tfidf = gensim.models.LdaMulticore(corpus=tf_idf_corpus,
#                                        id2word=id2word,
#                                        num_topics=best_num_topics,
#                                       random_state = np.random.RandomState(42))

# topic_model = BERTopic()
# topics, probs = topic_model.fit_transform(data_words)
# topic_distr, _ = topic_model.approximate_distribution(data_words)
# topic_distr_list = list()
# for topic_probs in topic_distr:
#     topic_distr_list.append([(i, topic_prob) for i,topic_prob in enumerate(topic_probs)])


# lda_model.get_document_topics(id2word.doc2bow(data_words[0]), minimum_probability=0.0)

new_df = pd.read_csv("/home/shaunaks/Explicit-Video-Segment-Classifier-and-Summarizer/runs/attention_fusion_default_networks_pairwise_21epochs/predictions_test.csv")

"""## Get the embeddings of LDA from the captions and train"""

def prepare_data_and_train(model, model_name, topic, params, prev_model_name):
    print(f'Preparing data and training for model {model_name} with topics {topic} and params {params}...')
    dataset = pd.DataFrame(columns=['topics', 'ans'])
    for i in range(331):
        l1 = []
        temp = model.get_document_topics(corpus[i], minimum_probability=0.0)
        for j in range(len(temp)):
            l1.append(temp[j][1])
        if new_df.iloc[i, 2] == 'explicit':
            a = 1
        else:
            a = 0
        dataset.loc[i] = [l1, a]

    X = dataset.topics.to_list()
    y = dataset.ans.to_list()

    coherence_model_lda = CoherenceModel(model=model, texts=data_words, dictionary=id2word, coherence='c_v')
    coherence_lda = coherence_model_lda.get_coherence()
    if prev_model_name==model_name:
        str_ = f'For model {model_name} with number of topics as {model.num_topics} and num params per layer {params}'
    else:
        str_ = f'\n For model {model_name} with number of topics as {model.num_topics} and num params per layer {params}'
    # print(str_)
    # print('Coherence Score: ', coherence_lda)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)
    clf = MLPClassifier(hidden_layer_sizes = (params[0], params[1]), max_iter=2000, random_state=42)
    clf.fit(X_train, y_train)

    y_pred = clf.predict(X_train)

    # print("Train accuracy: ", accuracy_score(y_train, y_pred))
    # print("Train F1: ", f1_score(y_train, y_pred))

    y_pred = clf.predict(X_test)
    test_accuracy = accuracy_score(y_test, y_pred)
    test_f1 = f1_score(y_test, y_pred)
    # print("Test accuracy: ", test_accuracy)
    # print("Test F1: ", test_f1)
    print('Preparing and training done')
    return test_f1, test_accuracy


#prev_model_name = ''
#data = list()
# num_params = [(20, 10), (64, 32), (100, 100), (250, 250), (400, 300)]
# #num_params = [(64, 32), (100, 100), (250, 250)]
# for topic, model_name, model in zip([10, max_topic, 150, 10], ['baseline', 'optimal topic model','arbitrarily high number of topics model','asymmetric_alpha_n_eta'], [lda_model_copy, lda_model, lda_model_more_topics, lda_model_copy_asymmetric]):
#     for params in num_params:
#         test_f1_acc = prepare_data_and_train(model, model_name, topic, params, prev_model_name)
#         if model_name=='asymmetric_alpha_n_eta':
#             data.append({'topic':f'Topic : {topic}_asymm', 'params':f'{params}', 'test_f1':test_f1_acc[0]})
#         else:
#             data.append({'topic':f'Topic : {topic}', 'params':f'{params}', 'test_f1':test_f1_acc[0]})


def makedir(dir_):
    os.makedirs(dir_, exist_ok=True)

#Grid search to find the best parameters
def grid_search(model, model_name, topic, corpus, id2word, model_topic_param_dir, logger_):
    st_time = time.time()
    dataset = pd.DataFrame(columns=['topics', 'ans'])
    for i in range(331):
        l1 = []
        temp = model.get_document_topics(corpus[i], minimum_probability=0.0)
        for j in range(len(temp)):
            l1.append(temp[j][1])
        if new_df.iloc[i, 2] == 'explicit':
            a = 1
        else:
            a = 0
        dataset.loc[i] = [l1, a]

    X = dataset.topics.to_list()
    y = dataset.ans.to_list()
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)
    parameter_space = {
            'hidden_layer_sizes': [(i, j) for i in range(10, 500, 10) for j in range(10, 500, 10)],
            'max_iter':[i for i in range(100, 5000, 100)],
            'solver':['sgd','adam','lbfgs'],
            'alpha':[0.0001, 0.001, 0.01, 0.1],
            'learning_rate':['constant', 'invscaling', 'adaptive'],
            'learning_rate_init': [0.001, 0.01, 0.1],
            'early_stopping':[True, False],
            'n_iter_no_change':[i for i in range(100, 4000, 100)],
            'tol': [1e-2, 1e-3, 1e-4, 1e-5]
        }    
    # parameter_space = {
    #         'hidden_layer_sizes': [(10, 20),(20,100)],
    #         'max_iter':[i for i in range(10, 20)],
    #         'solver':['sgd','adam','lbfgs'],
    #         'alpha':[0.0001],
    #         'learning_rate':['constant'],
    #         'learning_rate_init': [0.001, 0.01],
    #         'early_stopping':[True, False],
    #     }    

    clf = MLPClassifier(random_state=42)
    X_combined = X_train + X_test
    y_combined = y_train + y_test
    test_fold = np.array([-1 for i in range(len(X_train))] + [0 for i in range(len(X_test))])
    logger_.info(f'Staring random search for model {model_name} with topics {topic}...')
    max_num_iterations = 1
    grid = RandomizedSearchCV(clf, parameter_space, n_iter = max_num_iterations, cv=PredefinedSplit(test_fold), n_jobs=-1, scoring='f1', random_state = 42)
    grid.fit(X_combined, y_combined)

    best_model = grid.best_estimator_
    best_model_params = best_model.coefs_
    #import pdb;pdb.set_trace()
    y_pred_test = best_model.predict(X_test)
    best_params = grid.best_params_
    test_accuracy = accuracy_score(y_test, y_pred_test)
    test_f1 = f1_score(y_test, y_pred_test)
    best_params.update({'test_f1':test_f1, 'test_accuracy':test_accuracy})

    #import pdb; pdb.set_trace()
    logger_.info(f'For num topics {topic} and model {model_name}')
    logger_.info("Best parameters found: %s",best_params )
    logger_.info("Test Accuracy: %f", test_accuracy)
    logger_.info("Test F1 Score: %f", test_f1)

    model_path = os.path.join(model_topic_param_dir, f'best_model_{model_name}_test_f1_{test_f1}.pkl')
    param_details_path = os.path.join(model_topic_param_dir, f'best_params_{model_name}_test_f1_{test_f1}.json')
    model_params_path = os.path.join(model_topic_param_dir, f'best_model_params_{model_name}_test_f1_{test_f1}')
    dump(best_model, model_path)
    dump(best_model_params, model_params_path)
    json.dump(best_params, open(param_details_path, 'w'))
    time_taken_log = f'Time taken for model {model_name} with topics {topic} is {time.time()-st_time} seconds for {max_num_iterations} iterations \n\n'
    print(time_taken_log)
    logger_.info(time_taken_log)
    


curr_dir = os.getcwd()
gridsearch_root_dir = os.path.join(curr_dir, 'lda_gridsearch_experiments')
if os.path.exists(gridsearch_root_dir):
    shutil.rmtree(gridsearch_root_dir)

makedir(gridsearch_root_dir)
for topic, model_name, model in zip([81, 7], ['tf_idf_model', 'bertopic'], [lda_model_tfidf, topic_distr_list]):
    model_dir = os.path.join(gridsearch_root_dir,f'{model_name}')
    makedir(model_dir)
    model_topic_param_dir = os.path.join(model_dir,f'{model_name}_{topic}')
    makedir(model_topic_param_dir)
    logger_ = setup_logger(f'{model_name}_{topic}', model_topic_param_dir)
    grid_search(model, model_name, topic, corpus, id2word, model_topic_param_dir, logger_)